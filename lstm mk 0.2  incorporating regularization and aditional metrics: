import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
from IPython.display import clear_output

# Load your data from the file
file_path = '/content/ABbid_prices.txt'

# Read the data into a pandas DataFrame
data = pd.read_csv(file_path, header=None)

# Assuming the data contains only one column of time series values
time_series_data = data.iloc[:, 0].values.reshape(-1, 1)

# Normalize the data using Min-Max scaling
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(time_series_data)

# Define window size and split data into input sequences
window_size = 10

# Convert data to PyTorch tensors
scaled_data_tensor = torch.Tensor(scaled_data)

# Define LSTM model with regularization (L2)
class LSTMModel(nn.Module):
    def __init__(self):
        super(LSTMModel, self).__init__()
        self.lstm = nn.LSTM(input_size=1, hidden_size=50, batch_first=True)
        self.fc = nn.Linear(50, 10)  # Adjust output size to match the target shape
        self.dropout = nn.Dropout(0.2)  # Dropout regularization
    
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.fc(lstm_out[:, -1, :])
        return output

model = LSTMModel()
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization

# Train the model with real-time plotting
for i in range(len(scaled_data) - window_size):  # Loop through each window
    input_sequences = scaled_data_tensor[i:i+window_size]
    target_values = scaled_data_tensor[i+window_size:i+window_size+10]
    
    # Reshape input sequences for LSTM input (samples, time steps, features)
    input_sequences = input_sequences.view(1, window_size, 1)
    target_values = target_values.view(1, 10)  # Reshape target values to match model output
    
    # Train the model for one epoch
    optimizer.zero_grad()
    outputs = model(input_sequences)
    loss = criterion(outputs, target_values)
    loss.backward()
    optimizer.step()
    
    # Calculate additional metrics
    with torch.no_grad():
        forecasted_values = model(input_sequences)
        forecasted_values = scaler.inverse_transform(forecasted_values.view(-1, 1).detach().numpy())
        target_values_np = scaler.inverse_transform(target_values.view(-1, 1).detach().numpy())
        
        mae = mean_absolute_error(target_values_np, forecasted_values)
        mse = mean_squared_error(target_values_np, forecasted_values)
        r2 = r2_score(target_values_np, forecasted_values)
        print(f"Epoch [{i+1}/{len(scaled_data) - window_size}] - Loss: {loss.item()} - MAE: {mae} - MSE: {mse} - R2: {r2}")
    
    # Clear the previous output
    clear_output(wait=True)
    
    # Plot input sequences, target values, and forecasted values for each window
    plt.figure(figsize=(10, 6))
    plt.plot(range(window_size), scaler.inverse_transform(input_sequences.view(window_size, 1).detach().numpy()), label='Input Sequences', marker='o')
    plt.plot(range(window_size, window_size + 10), target_values_np, label='Target Values')
    plt.plot(range(window_size, window_size + 10), forecasted_values, label='Forecasted Values', linestyle='--')
    plt.legend()
    plt.title(f'Window {i+1}')
    plt.show()
