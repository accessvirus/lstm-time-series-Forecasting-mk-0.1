import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
from IPython.display import clear_output

# Placeholder for loading data from a file
# Replace this with your code for loading data from a file
file_path = '/path/to/your/data/file.csv'
data = pd.read_csv(file_path, header=None)
time_series_data = data.iloc[:, 0]

# Calculate the moving average with window size 5 and fill NaN values with the mean of the series
moving_avg = time_series_data.rolling(window=14, min_periods=1).mean().fillna(time_series_data.mean())

# Normalize the data using Min-Max scaling after applying the moving average
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(moving_avg.values.reshape(-1, 1))

# Convert data to PyTorch tensors
scaled_data_tensor = torch.Tensor(scaled_data)

def train_lstm_model_dwa(scaled_data_tensor, window_size=10, target_length=10, learning_rate=0.001, weight_decay=1e-5, epochs=50):
    class LSTMModel(nn.Module):
        def __init__(self):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size=1, hidden_size=512, num_layers=3, batch_first=True)
            self.fc = nn.Linear(512, 10)
            self.dropout = nn.Dropout(0.2)

        def forward(self, x):
            lstm_out, _ = self.lstm(x)
            output = self.fc(lstm_out[:, -1, :])
            return output

    model = LSTMModel()
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)

    num_averages = 5  # Number of weight averages
    avg_weights = [p.clone().detach() for p in model.parameters()]  # Initialize weights

    for i in range(len(scaled_data_tensor) - window_size - target_length):
        input_sequences = scaled_data_tensor[i:i + window_size]
        target_values = scaled_data_tensor[i + window_size:i + window_size + target_length]

        input_sequences = input_sequences.view(1, window_size, 1)
        target_values = target_values.view(1, target_length)

        optimizer.zero_grad()
        outputs = model(input_sequences)
        loss = criterion(outputs, target_values)
        loss.backward()
        optimizer.step()

        if (i + 1) % (len(scaled_data_tensor) // num_averages) == 0:
            for p, avg_p in zip(model.parameters(), avg_weights):
                avg_p.mul_(i // (len(scaled_data_tensor) // num_averages) + 1).add_(p.data).div_(i // (len(scaled_data_tensor) // num_averages) + 2)

        with torch.no_grad():
            forecasted_values = model(input_sequences)
            forecasted_values = scaler.inverse_transform(forecasted_values.view(-1, 1).detach().numpy())
            target_values_np = scaler.inverse_transform(target_values.view(-1, 1).detach().numpy())

            mae = mean_absolute_error(target_values_np, forecasted_values)
            mse = mean_squared_error(target_values_np, forecasted_values)
            r2 = r2_score(target_values_np, forecasted_values)
            print(f"Epoch [{i+1}/{len(scaled_data_tensor) - window_size - target_length}] - Loss: {loss.item()} - MAE: {mae} - MSE: {mse} - R2: {r2}")

        clear_output(wait=True)

        plt.figure(figsize=(10, 6))
        plt.plot(range(window_size), scaler.inverse_transform(input_sequences.view(window_size, 1).detach().numpy()), label='Input Sequences', marker='o')
        plt.plot(range(window_size, window_size + target_length), target_values_np, label='Target Values')
        plt.plot(range(window_size, window_size + target_length), forecasted_values, label='Forecasted Values', linestyle='--')
        plt.legend()
        plt.title(f'Window {i+1}')
        plt.show()

    for p, avg_p in zip(model.parameters(), avg_weights):
        p.data.copy_(avg_p)

# Train the LSTM model with Dynamic Weight Averaging
train_lstm_model_dwa(scaled_data_tensor)
